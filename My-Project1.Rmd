---
title: "Project 1"
author: "Skyler Moomey"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    csl: biomed-central.csl
    df_print: paged
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    number_sections: yes
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
  pdf_document:
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    toc: yes
    toc_depth: 4
#bibliography: project.bib
abstract: Four predictors of software defects were created by researchers at West Virginia University. Each measurement was given an upper bound. If any predictor was measured to be higher than its upper bound, that piece of software was predicted to have an error. All such measurements were recorded in SWDEFECTS.csv, along with their true defective status. This study finds that the predictors of defects tended to be quite accurate, but also gave many false alarms.
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to the Data

## Data and Variables
In order to predict the reliability of large sets of software, engineers and computer scientists developed
several algorithms. These four algorithms measure four different attributes of a given file, namely: lines of code, cyclomatic complexity, essential complexity, and design complexity. If these attributes exceed
a given upper bound, different for each measurement, the algorithm predicts that file to have a defect. This data
is stored in SWDEFECTS.csv.

## Summary Table 

<center>
Defect Predictions v Actual
</center>
```{r}
#all data in table
swdefects.tab <- read.csv("SWDEFECTS.csv")


#total two-way summary table
predictions <- with(swdefects.tab, (predict.vg.10 == "yes") | (predict.evg.14.5 == "yes")
                                      | (predict.ivg.9.2 == "yes") | (predict.loc.50 == "yes"))

#takes a 4x4 table and a grand total and creates a two way summary table
twowaysum <- function(tab, total)
{
  mat = matrix(c(tab[1, 1], tab[1, 2], I(tab[1,1] + tab[1,2])
                 ,tab[2, 1], tab[2,2], I(tab[2,1] + tab[2, 2])
                 , I(tab[1,1] + tab[2,1]), I(tab[1,2] + tab[2,2]), total)
               , ncol=3,byrow=TRUE)
  colnames(mat) = c("No Defect", "Defect", "Row Totals")
  rownames(mat) = c("Not Predicted", "Predicted", "Column Totals")
  mat
}

totals.tab <- table(predictions, swdefects.tab$defect)

twowaysum(totals.tab, nrow(swdefects.tab))
```


## Formulae

1. Accuracy: $P($Algorithm is correct$)=\frac{(a+d)}{(a+b+c+d)}\\$
2. Detection rate: $P($predict defect|module has defect$)=\frac{d}{(b+d)}\\$
3. False alarm rate: $P($predict defect|module has no defect$)=\frac{c}{(a+c)}\\$
4. Precision: $P($module has defect|predict defect$)=\frac{d}{(c+d)}$

# R functions
R functions for the same probability measures:
```{r accuracy}
acc=function(a,b,c,d)
{
  accuracy = (a+d)/(a+b+c+d)
  accuracy
}
```

```{r detecton}
detect=function(b,d)
{
  detection=d/(b+d)
  detection
}
```

```{r alarm}
falarm=function(a,c)
{
  false_alarm=c/(a+c)
  false_alarm
}
```

```{r precision}
prec=function(c,d)
{
  precision=d/(c+d)
  precision
}
```


# Tables in Figure SIA3.1
Two-way Summary tables for each predictive factor.

Lines of Code v Defect:
```{r }
#loc v defect

loc.tab <- with(swdefects.tab, table(predict.loc.50, defect))

twowaysum(loc.tab, nrow(swdefects.tab))

barplot(loc.tab, beside=TRUE, leg=TRUE, col=4:5,
        ylab="Number of Predictions", xlab="Defect Present")
title(main= "Lines of Code Test")
```

Cyclomatic Complexity v Defect:
```{r}
#cyclomatic complexity v defect
cyclo.tab <- with(swdefects.tab, table(predict.vg.10, defect))

twowaysum(cyclo.tab, nrow(swdefects.tab))

barplot(cyclo.tab, beside=TRUE, leg=TRUE, col=6:7,
        ylab="Number of Predictions", xlab="Defect Present")
title(main="Cyclomatic Complexity Test")
```

Essential Complexity v Defect:
```{r}
#essential complexity v defect
evg.tab <- with(swdefects.tab, table(predict.evg.14.5, defect))

twowaysum(evg.tab, nrow(swdefects.tab))

barplot(evg.tab, beside=TRUE, leg=TRUE, col=11:12,
        ylab="Number of Predictions", xlab="Defect Present")
title(main="Essential Complexity Test")
```

Design Complexity v Defect:
```{r}
#design complexity v defect
ivg.tab <- with(swdefects.tab, table(predict.ivg.9.2, defect))

twowaysum(ivg.tab, nrow(swdefects.tab))

barplot(ivg.tab, beside=TRUE, leg=TRUE, col=2:3,
        ylab="Number of Predictions", xlab="Defect Present")
title(main="Design Complexity Test")
```


# Table and Barplot


## Table Page 127
```{r}
#function to calculate accuracy, detection rate,
#false alarm rate, precision
prob_measure=function(summary.tab)
{
  a=summary.tab[1,1]
  b=summary.tab[1,2]
  c=summary.tab[2,1]
  d=summary.tab[2,2]
  
  accuracy=acc(a, b, c, d)
  accuracy=round(accuracy, 3)
  detection=detect(b, d)
  detection=round(detection,3)
  false_alarm=falarm(a, c)
  false_alarm=round(false_alarm, 3)
  precision=prec(c, d)
  precision=round(precision, 3)
  
  as.list(c(accuracy, detection, false_alarm, precision))
}

##collect measures and add title for gt
loc_measures <- c("Lines of Code", prob_measure(loc.tab))
cyclo_measures <-c("Cyclomatic Complexity", prob_measure(cyclo.tab))
ivg_measures <- c("Design Complexity", prob_measure(ivg.tab))
evg_measures <- c("Essential Complexity",prob_measure(evg.tab))

#table to store all values
measures <- matrix(c(loc_measures, cyclo_measures,                         evg_measures, ivg_measures)
                   ,nrow=4, ncol=5, byrow=TRUE)

##library I found online for making prettier tables
library(gt)
measures <- as.data.frame(measures)
gt_tbl <- gt(data=measures)

gt_tbl <-
  
  gt_tbl %>%
  tab_header(
    title="Probability Measures for Evaluating Defect 
    Prediction Algorithms",
  ) %>%
  cols_label(
    V1="Method",
    V2="Accuracy", V3="Detection Rate",
    V4="False Alarm Rate", V5="Precision"
  ) 
gt_tbl
```


## Barplot of Data
```{r}
mybar=function(tab, accuracy){
  
  bar.plot <- barplot(tab, main="Probability Measures"
                  , ylab="Percent %"
                  ,legend = rownames(tab)
                  ,beside=TRUE, col=rainbow(4))
  
  bar.plot
  
  #return list
  as.list(tab)
}

#lists of measures for each test
loc_measures <-  prob_measure(loc.tab)
cyclo_measures <- prob_measure(cyclo.tab)
ivg_measures <- prob_measure(ivg.tab)
evg_measures <-  prob_measure(evg.tab)

#names for rows in table
rows <- c("Lines of Code", "Cyclomatic Complexity"
           ,"Essential Complexity", "Design Complexity")

#names for columns in table
cols <- c("Accuracy", "Detection Rate", "False Alarm Rate", "Precision")

#combine measurement lists into a table
measures.tab <- matrix(c(loc_measures, cyclo_measures
                         ,evg_measures, ivg_measures)
                       ,nrow=4,ncol=4, byrow=TRUE)

mode(measures.tab) = "numeric"
measures.tab <- as.table(measures.tab)

#assign names
colnames(measures.tab) <- cols
row.names(measures.tab) <- rows

mybar(measures.tab, 3)
```

While in general the predictors of software defects produced by West Virginia University researchers is quite accurate, giving correct answers around 80% of the time. However, given that the software has a defect, the predictors only detect the defect around 40% of the time. Given that the point of the predictors is to detect software defect, it seems that the predictors need to be developed much further, to ensure a better detection rate. Additionally, the software gives false alarms around 10% of the time. Finally, their precisions, that is, how often the module actually has a defect when the predictors detect one, is only about 33%, far too low a measure.

